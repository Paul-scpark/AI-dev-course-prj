{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6342cf7c-7e0c-4caf-8e8b-8d521a05353e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-05T17:57:12.685903Z",
     "iopub.status.busy": "2023-02-05T17:57:12.685577Z",
     "iopub.status.idle": "2023-02-05T17:57:15.208295Z",
     "shell.execute_reply": "2023-02-05T17:57:15.207819Z",
     "shell.execute_reply.started": "2023-02-05T17:57:12.685781Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-06 02:57:14.608557: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "import gluonnlp as nlp\n",
    "from tqdm import tqdm\n",
    "\n",
    "from transformers import AutoModel, AutoTokenizer, RobertaTokenizer, AutoModelForSequenceClassification\n",
    "from transformers import AutoModelForMaskedLM, RobertaForSequenceClassification\n",
    "\n",
    "from transformers import AdamW\n",
    "from transformers.optimization import get_cosine_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "02e84216-7754-496e-8d56-a99fab22cb81",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-05T17:57:15.209265Z",
     "iopub.status.busy": "2023-02-05T17:57:15.208871Z",
     "iopub.status.idle": "2023-02-05T17:57:15.214959Z",
     "shell.execute_reply": "2023-02-05T17:57:15.214555Z",
     "shell.execute_reply.started": "2023-02-05T17:57:15.209237Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7091f476-d19d-47d4-b1f4-db3d4d0b7397",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-05T17:57:15.215521Z",
     "iopub.status.busy": "2023-02-05T17:57:15.215393Z",
     "iopub.status.idle": "2023-02-05T17:57:15.217615Z",
     "shell.execute_reply": "2023-02-05T17:57:15.217299Z",
     "shell.execute_reply.started": "2023-02-05T17:57:15.215508Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a1989091-a2eb-4fda-ad95-98e0746592ad",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-05T17:57:15.218934Z",
     "iopub.status.busy": "2023-02-05T17:57:15.218806Z",
     "iopub.status.idle": "2023-02-05T17:57:15.306022Z",
     "shell.execute_reply": "2023-02-05T17:57:15.305500Z",
     "shell.execute_reply.started": "2023-02-05T17:57:15.218921Z"
    }
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import torch\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8dc1ff24-7c2f-4de4-a994-8dc8ff7ecc03",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-05T17:57:15.306830Z",
     "iopub.status.busy": "2023-02-05T17:57:15.306564Z",
     "iopub.status.idle": "2023-02-05T17:57:15.308955Z",
     "shell.execute_reply": "2023-02-05T17:57:15.308587Z",
     "shell.execute_reply.started": "2023-02-05T17:57:15.306817Z"
    }
   },
   "outputs": [],
   "source": [
    "gpu_id = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ab6bb260-8d84-4ddd-9b06-5413fa967f21",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-05T17:57:15.309610Z",
     "iopub.status.busy": "2023-02-05T17:57:15.309468Z",
     "iopub.status.idle": "2023-02-05T17:57:15.312273Z",
     "shell.execute_reply": "2023-02-05T17:57:15.311925Z",
     "shell.execute_reply.started": "2023-02-05T17:57:15.309598Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = (\n",
    "        torch.device(\"cpu\")\n",
    "        if gpu_id < 0\n",
    "        else torch.device(\"cuda:%d\" % gpu_id)\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "16534b65-a1e7-48c0-8670-d646be0c6065",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-05T17:57:15.312925Z",
     "iopub.status.busy": "2023-02-05T17:57:15.312772Z",
     "iopub.status.idle": "2023-02-05T17:57:15.316500Z",
     "shell.execute_reply": "2023-02-05T17:57:15.316051Z",
     "shell.execute_reply.started": "2023-02-05T17:57:15.312913Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# HUGGINGFACE_MODEL_PATH = \"rurupang/roberta-base-finetuned-sts\"\n",
    "# HUGGINGFACE_MODEL_PATH = \"klue/roberta-base\"\n",
    "HUGGINGFACE_MODEL_PATH = \"klue/roberta-small\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dda5c925-8bb8-4d5f-9d91-8c36f692006e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-05T17:57:15.317157Z",
     "iopub.status.busy": "2023-02-05T17:57:15.316934Z",
     "iopub.status.idle": "2023-02-05T17:57:20.309821Z",
     "shell.execute_reply": "2023-02-05T17:57:20.309230Z",
     "shell.execute_reply.started": "2023-02-05T17:57:15.317144Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# roberta_tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(HUGGINGFACE_MODEL_PATH)\n",
    "\n",
    "# model = AutoModelForSequenceClassification.from_pretrained(\"rurupang/roberta-base-finetuned-sts\")\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"rurupang/roberta-base-finetuned-sts\", use_fast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e40e3167-9da8-423f-a637-410933e8f8ff",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-05T17:57:20.310550Z",
     "iopub.status.busy": "2023-02-05T17:57:20.310409Z",
     "iopub.status.idle": "2023-02-05T17:57:20.989062Z",
     "shell.execute_reply": "2023-02-05T17:57:20.988528Z",
     "shell.execute_reply.started": "2023-02-05T17:57:20.310537Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"/home/keti/Desktop/eunbyeol/Workspace/Data-planet/ETRI/train.csv\")\n",
    "test_df = pd.read_csv(\"/home/keti/Desktop/eunbyeol/Workspace/Data-planet/ETRI/test.csv\")\n",
    "\n",
    "train_df = train_df.dropna(axis=0)\n",
    "test_df = test_df.dropna(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "da8af5da-86bb-4178-85c3-a7b3e7325496",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-05T17:57:20.989849Z",
     "iopub.status.busy": "2023-02-05T17:57:20.989708Z",
     "iopub.status.idle": "2023-02-05T17:57:21.012773Z",
     "shell.execute_reply": "2023-02-05T17:57:21.012286Z",
     "shell.execute_reply.started": "2023-02-05T17:57:20.989836Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>url</th>\n",
       "      <th>data_type</th>\n",
       "      <th>source</th>\n",
       "      <th>ori_label</th>\n",
       "      <th>ori_source</th>\n",
       "      <th>data_platform_id</th>\n",
       "      <th>label</th>\n",
       "      <th>encoded_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>서울시 강동구 집단급식소식품판매업 인허가 정보</td>\n",
       "      <td>강동구의 집단급식소를 대상으로 식품을 판매하는 업소정보* 좌표안내 : 중부원점TM(...</td>\n",
       "      <td>https://data.seoul.go.kr/dataList/OA-18051/S/1...</td>\n",
       "      <td>SHEETOpenAPI</td>\n",
       "      <td>강동구 스마트도시정책관 빅데이터담당관</td>\n",
       "      <td>보건</td>\n",
       "      <td>서울데이터광장</td>\n",
       "      <td>3</td>\n",
       "      <td>보건의료</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>경상남도 함양군_지방세 납부 현황</td>\n",
       "      <td>신용카드 가상계좌 등 지방세 납부매체별 납부현황으로 납부년도 납부매체 납부매체전자고...</td>\n",
       "      <td>https://www.data.go.kr/data/15079319/fileData.do</td>\n",
       "      <td>CSV</td>\n",
       "      <td>공공데이터포털</td>\n",
       "      <td>경제금융</td>\n",
       "      <td>빅데이터지도</td>\n",
       "      <td>1</td>\n",
       "      <td>경제금융</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>부산광역시 남구_하천점용현황_20210927</td>\n",
       "      <td>부산광역시 남구 하천 점용 현황에 대한 데이터로 유형 하천명 소재지 면적  점용시작...</td>\n",
       "      <td>https://www.data.go.kr/data/15047963/fileData.do</td>\n",
       "      <td>텍스트</td>\n",
       "      <td>부산광역시 남구</td>\n",
       "      <td>일반공공행정</td>\n",
       "      <td>공공데이터포털</td>\n",
       "      <td>0</td>\n",
       "      <td>행정법률</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>한국주택금융공사_일정관리관련내역</td>\n",
       "      <td>한국주택금융공사 채권관리부 일정관리관련 내역이 포함되어 있습니다. (해당 부서의 업...</td>\n",
       "      <td>https://www.data.go.kr/data/15072798/fileData.do</td>\n",
       "      <td>CSV</td>\n",
       "      <td>공공데이터포털</td>\n",
       "      <td>환경자원</td>\n",
       "      <td>빅데이터지도</td>\n",
       "      <td>1</td>\n",
       "      <td>환경기상</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>충청남도_노인의료복지시설 현황_20211231</td>\n",
       "      <td>2019년 노인 요양시설 및 요양공동생활가정에 관한 지역 시설 종류 시설명 연락처 ...</td>\n",
       "      <td>https://www.data.go.kr/data/15044627/fileData.do</td>\n",
       "      <td>텍스트</td>\n",
       "      <td>충청남도</td>\n",
       "      <td>사회복지</td>\n",
       "      <td>공공데이터포털</td>\n",
       "      <td>0</td>\n",
       "      <td>사회복지</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       title  \\\n",
       "0  서울시 강동구 집단급식소식품판매업 인허가 정보   \n",
       "1         경상남도 함양군_지방세 납부 현황   \n",
       "2   부산광역시 남구_하천점용현황_20210927   \n",
       "4          한국주택금융공사_일정관리관련내역   \n",
       "5  충청남도_노인의료복지시설 현황_20211231   \n",
       "\n",
       "                                         description  \\\n",
       "0  강동구의 집단급식소를 대상으로 식품을 판매하는 업소정보* 좌표안내 : 중부원점TM(...   \n",
       "1  신용카드 가상계좌 등 지방세 납부매체별 납부현황으로 납부년도 납부매체 납부매체전자고...   \n",
       "2  부산광역시 남구 하천 점용 현황에 대한 데이터로 유형 하천명 소재지 면적  점용시작...   \n",
       "4  한국주택금융공사 채권관리부 일정관리관련 내역이 포함되어 있습니다. (해당 부서의 업...   \n",
       "5  2019년 노인 요양시설 및 요양공동생활가정에 관한 지역 시설 종류 시설명 연락처 ...   \n",
       "\n",
       "                                                 url     data_type  \\\n",
       "0  https://data.seoul.go.kr/dataList/OA-18051/S/1...  SHEETOpenAPI   \n",
       "1   https://www.data.go.kr/data/15079319/fileData.do           CSV   \n",
       "2   https://www.data.go.kr/data/15047963/fileData.do           텍스트   \n",
       "4   https://www.data.go.kr/data/15072798/fileData.do           CSV   \n",
       "5   https://www.data.go.kr/data/15044627/fileData.do           텍스트   \n",
       "\n",
       "                 source ori_label ori_source  data_platform_id label  \\\n",
       "0  강동구 스마트도시정책관 빅데이터담당관        보건    서울데이터광장                 3  보건의료   \n",
       "1               공공데이터포털      경제금융     빅데이터지도                 1  경제금융   \n",
       "2              부산광역시 남구    일반공공행정    공공데이터포털                 0  행정법률   \n",
       "4               공공데이터포털      환경자원     빅데이터지도                 1  환경기상   \n",
       "5                  충청남도      사회복지    공공데이터포털                 0  사회복지   \n",
       "\n",
       "   encoded_label  \n",
       "0              5  \n",
       "1              0  \n",
       "2             12  \n",
       "4             13  \n",
       "5              6  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(train_df['label'])\n",
    "num_labels = len(label_encoder.classes_)\n",
    "\n",
    "train_df['encoded_label'] = np.asarray(label_encoder.transform(train_df['label']), dtype=np.int32)\n",
    "train_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "22b0397d-32df-4be4-bef8-d852a3911d94",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-05T17:57:21.013444Z",
     "iopub.status.busy": "2023-02-05T17:57:21.013301Z",
     "iopub.status.idle": "2023-02-05T17:57:21.022545Z",
     "shell.execute_reply": "2023-02-05T17:57:21.022063Z",
     "shell.execute_reply.started": "2023-02-05T17:57:21.013431Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>url</th>\n",
       "      <th>data_type</th>\n",
       "      <th>source</th>\n",
       "      <th>ori_label</th>\n",
       "      <th>ori_source</th>\n",
       "      <th>data_platform_id</th>\n",
       "      <th>label</th>\n",
       "      <th>encoded_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>서울특별시 동작구_체육시설업 현황_20220719</td>\n",
       "      <td>이 데이터는 서울특별시 동작구 체육시설업 현황에 관한 것입니다. 이 데이터에는 업종...</td>\n",
       "      <td>https://www.data.go.kr/data/15016523/fileData.do</td>\n",
       "      <td>텍스트</td>\n",
       "      <td>서울특별시 동작구</td>\n",
       "      <td>문화체육관광</td>\n",
       "      <td>공공데이터포털</td>\n",
       "      <td>0</td>\n",
       "      <td>문화관광</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>국가보훈처_세종특별자치시 읍면동별 보훈대상자 인원현황</td>\n",
       "      <td>세종특별자치시 시군구별 보훈대상자 인원현황 자료1. 적용 대상 국가유공자는 「국가유...</td>\n",
       "      <td>https://www.data.go.kr/data/15098581/fileData.do</td>\n",
       "      <td>CSV</td>\n",
       "      <td>공공데이터포털</td>\n",
       "      <td>행정법률</td>\n",
       "      <td>빅데이터지도</td>\n",
       "      <td>1</td>\n",
       "      <td>행정법률</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>한국주택금융공사_계좌별누적회수이자정보(회계기간_해당결산월말)_20200901</td>\n",
       "      <td>한국주택금융공사의 계좌별누적회수이자정보에 대한 데이터로 유동화계획코드 인수코드 회수...</td>\n",
       "      <td>https://www.data.go.kr/data/15073239/fileData.do</td>\n",
       "      <td>텍스트</td>\n",
       "      <td>한국주택금융공사</td>\n",
       "      <td>사회복지</td>\n",
       "      <td>공공데이터포털</td>\n",
       "      <td>0</td>\n",
       "      <td>사회복지</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>법무부_56(국적) 연도별 국적 상실 이탈 처리 현황</td>\n",
       "      <td>대한민국 국적을 상실하거나 이탈 처리된 자의 현황을 연도별로 제공(연도 유형(상실 ...</td>\n",
       "      <td>https://www.data.go.kr/data/15100048/fileData.do</td>\n",
       "      <td>CSV</td>\n",
       "      <td>공공데이터포털</td>\n",
       "      <td>행정법률</td>\n",
       "      <td>빅데이터지도</td>\n",
       "      <td>1</td>\n",
       "      <td>행정법률</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>인천광역시 서구_쓰레기 무단투기 감시용 카메라(CCTV) 설치 현황_20220824</td>\n",
       "      <td>인천광역시 서구의 쓰레기 무단투기 감시용 카메라(CCTV) 설치 현황에 관한 데이터...</td>\n",
       "      <td>https://www.data.go.kr/data/15105069/fileData.do</td>\n",
       "      <td>텍스트</td>\n",
       "      <td>인천광역시 서구</td>\n",
       "      <td>환경</td>\n",
       "      <td>공공데이터포털</td>\n",
       "      <td>0</td>\n",
       "      <td>환경기상</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            title  \\\n",
       "0                     서울특별시 동작구_체육시설업 현황_20220719   \n",
       "1                   국가보훈처_세종특별자치시 읍면동별 보훈대상자 인원현황   \n",
       "2      한국주택금융공사_계좌별누적회수이자정보(회계기간_해당결산월말)_20200901   \n",
       "3                   법무부_56(국적) 연도별 국적 상실 이탈 처리 현황   \n",
       "4  인천광역시 서구_쓰레기 무단투기 감시용 카메라(CCTV) 설치 현황_20220824   \n",
       "\n",
       "                                         description  \\\n",
       "0  이 데이터는 서울특별시 동작구 체육시설업 현황에 관한 것입니다. 이 데이터에는 업종...   \n",
       "1  세종특별자치시 시군구별 보훈대상자 인원현황 자료1. 적용 대상 국가유공자는 「국가유...   \n",
       "2  한국주택금융공사의 계좌별누적회수이자정보에 대한 데이터로 유동화계획코드 인수코드 회수...   \n",
       "3  대한민국 국적을 상실하거나 이탈 처리된 자의 현황을 연도별로 제공(연도 유형(상실 ...   \n",
       "4  인천광역시 서구의 쓰레기 무단투기 감시용 카메라(CCTV) 설치 현황에 관한 데이터...   \n",
       "\n",
       "                                                url data_type     source  \\\n",
       "0  https://www.data.go.kr/data/15016523/fileData.do       텍스트  서울특별시 동작구   \n",
       "1  https://www.data.go.kr/data/15098581/fileData.do       CSV    공공데이터포털   \n",
       "2  https://www.data.go.kr/data/15073239/fileData.do       텍스트   한국주택금융공사   \n",
       "3  https://www.data.go.kr/data/15100048/fileData.do       CSV    공공데이터포털   \n",
       "4  https://www.data.go.kr/data/15105069/fileData.do       텍스트   인천광역시 서구   \n",
       "\n",
       "  ori_label ori_source  data_platform_id label  encoded_label  \n",
       "0    문화체육관광    공공데이터포털                 0  문화관광              4  \n",
       "1      행정법률     빅데이터지도                 1  행정법률             12  \n",
       "2      사회복지    공공데이터포털                 0  사회복지              6  \n",
       "3      행정법률     빅데이터지도                 1  행정법률             12  \n",
       "4        환경    공공데이터포털                 0  환경기상             13  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df['encoded_label'] = np.asarray(label_encoder.transform(test_df['label']), dtype=np.int32)\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a9eb4d68-0e91-41c2-a8ab-c01f02bf1036",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-05T17:57:21.023178Z",
     "iopub.status.busy": "2023-02-05T17:57:21.023029Z",
     "iopub.status.idle": "2023-02-05T17:57:21.053538Z",
     "shell.execute_reply": "2023-02-05T17:57:21.053099Z",
     "shell.execute_reply.started": "2023-02-05T17:57:21.023164Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_texts = train_df[\"description\"].to_list() # Features (not-tokenized yet)\n",
    "train_labels = torch.nn.functional.one_hot(torch.tensor(train_df[\"encoded_label\"].to_list())) # Labels\n",
    "\n",
    "test_texts = test_df[\"description\"].to_list() # Features (not-tokenized yet)\n",
    "test_labels = torch.nn.functional.one_hot(torch.tensor(test_df[\"encoded_label\"].to_list())) # Labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c85c36a3-5055-4cc1-95de-f06a215efe07",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-05T17:57:21.055305Z",
     "iopub.status.busy": "2023-02-05T17:57:21.055111Z",
     "iopub.status.idle": "2023-02-05T17:57:21.058207Z",
     "shell.execute_reply": "2023-02-05T17:57:21.057833Z",
     "shell.execute_reply.started": "2023-02-05T17:57:21.055290Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([11501, 14])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_labels.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "85e863cc-1cb3-4f85-90fb-16a241d68a15",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-05T17:57:21.058865Z",
     "iopub.status.busy": "2023-02-05T17:57:21.058688Z",
     "iopub.status.idle": "2023-02-05T17:57:34.649276Z",
     "shell.execute_reply": "2023-02-05T17:57:34.648739Z",
     "shell.execute_reply.started": "2023-02-05T17:57:21.058851Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_encodings = tokenizer(train_texts, truncation=True, padding=True, return_tensors=\"pt\")\n",
    "test_encodings = tokenizer(test_texts, truncation=True, padding=True, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1ee8f956-eaab-4ba5-8548-bf05e1fc5167",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-05T17:57:34.649948Z",
     "iopub.status.busy": "2023-02-05T17:57:34.649813Z",
     "iopub.status.idle": "2023-02-05T17:57:34.654032Z",
     "shell.execute_reply": "2023-02-05T17:57:34.653711Z",
     "shell.execute_reply.started": "2023-02-05T17:57:34.649935Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    0, 19647,  2079,  ...,     1,     1,     1],\n",
       "        [    0, 11776,  7024,  ...,     1,     1,     1],\n",
       "        [    0,  3902, 16955,  ...,     1,     1,     1],\n",
       "        ...,\n",
       "        [    0,  4364, 20000,  ...,     1,     1,     1],\n",
       "        [    0, 29890,  4856,  ...,     1,     1,     1],\n",
       "        [    0,  9975,  2155,  ...,     1,     1,     1]]), 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0]])}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2e990cf2-e835-4905-b337-15de6b834883",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-05T17:57:34.654672Z",
     "iopub.status.busy": "2023-02-05T17:57:34.654503Z",
     "iopub.status.idle": "2023-02-05T17:57:34.658202Z",
     "shell.execute_reply": "2023-02-05T17:57:34.657858Z",
     "shell.execute_reply.started": "2023-02-05T17:57:34.654660Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    0, 19647,  2079,  ...,     1,     1,     1],\n",
       "        [    0, 11776,  7024,  ...,     1,     1,     1],\n",
       "        [    0,  3902, 16955,  ...,     1,     1,     1],\n",
       "        ...,\n",
       "        [    0,  4364, 20000,  ...,     1,     1,     1],\n",
       "        [    0, 29890,  4856,  ...,     1,     1,     1],\n",
       "        [    0,  9975,  2155,  ...,     1,     1,     1]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_encodings.input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dc056e93-b1e3-4afc-a48a-3e5288c2b563",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-05T17:57:34.658852Z",
     "iopub.status.busy": "2023-02-05T17:57:34.658693Z",
     "iopub.status.idle": "2023-02-05T17:57:34.661862Z",
     "shell.execute_reply": "2023-02-05T17:57:34.661539Z",
     "shell.execute_reply.started": "2023-02-05T17:57:34.658840Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "train_data = TensorDataset(train_encodings.input_ids, train_encodings.attention_mask, train_labels)\n",
    "train_dataloader = DataLoader(train_data, batch_size=8)\n",
    "\n",
    "test_data = TensorDataset(test_encodings.input_ids, test_encodings.attention_mask, test_labels)\n",
    "test_dataloader = DataLoader(test_data, batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0c6f78d9-4124-4c79-8341-8249b6d81a76",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-05T17:57:34.662536Z",
     "iopub.status.busy": "2023-02-05T17:57:34.662378Z",
     "iopub.status.idle": "2023-02-05T17:57:34.668465Z",
     "shell.execute_reply": "2023-02-05T17:57:34.668145Z",
     "shell.execute_reply.started": "2023-02-05T17:57:34.662524Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([    0, 19647,  2079,  4674,  9034,  2024,  2138,  3756,  6233,  4693,\n",
       "          2069,  3899,  2205,  2259, 10460, 19861,    14, 23740,  2283,  2369,\n",
       "            30,  9775,  2252,  2532,  2081,  2110,    12, 15760,  2238,  2341,\n",
       "            30, 22860,  2254,    13, 23740,  2418,  2170,  4340,  4162, 12765,\n",
       "          2079, 23740, 19861,  2052,  2307,  1485,  2382,  2119, 23740,  2259,\n",
       "          3894, 19521,  1513,  2118,  1380,  2053,    14,  1163,  5189,  2259,\n",
       "            23,  2210,  2165,  4208,  2138,  3894, 11800,    18,     2,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1]),\n",
       " tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "488b3086-5cfb-44a6-a629-8ce4697753c1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-05T17:57:34.669069Z",
     "iopub.status.busy": "2023-02-05T17:57:34.668904Z",
     "iopub.status.idle": "2023-02-05T17:57:36.603867Z",
     "shell.execute_reply": "2023-02-05T17:57:36.603447Z",
     "shell.execute_reply.started": "2023-02-05T17:57:34.669057Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at klue/roberta-small were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at klue/roberta-small and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# model = RobertaForSequenceClassification.from_pretrained(HUGGINGFACE_MODEL_PATH, return_dict=False)\n",
    "model = RobertaForSequenceClassification.from_pretrained(HUGGINGFACE_MODEL_PATH, num_labels=14, problem_type=\"multi_label_classification\", return_dict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a96d4742-6611-4577-8d87-a38fc08b7f57",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-05T17:57:36.604795Z",
     "iopub.status.busy": "2023-02-05T17:57:36.604426Z",
     "iopub.status.idle": "2023-02-05T17:57:38.577335Z",
     "shell.execute_reply": "2023-02-05T17:57:38.576839Z",
     "shell.execute_reply.started": "2023-02-05T17:57:36.604778Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaForSequenceClassification(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(32000, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): RobertaClassificationHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=768, out_features=14, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr = 2e-5, \n",
    "                  eps = 1e-8 \n",
    "                )\n",
    "\n",
    "# Number of training epochs\n",
    "epochs = 4\n",
    "\n",
    "# Total number of training steps is number of batches * number of epochs.\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "# Create the learning rate scheduler\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer,\n",
    "                                            num_warmup_steps = 0,\n",
    "                                            num_training_steps = total_steps)\n",
    "loss_fn = nn.CrossEntropyLoss(reduction='sum')\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f4aa6a09-052a-407d-8513-a65e2e610663",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-05T17:57:38.578055Z",
     "iopub.status.busy": "2023-02-05T17:57:38.577917Z",
     "iopub.status.idle": "2023-02-05T17:57:38.580755Z",
     "shell.execute_reply": "2023-02-05T17:57:38.580407Z",
     "shell.execute_reply.started": "2023-02-05T17:57:38.578042Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0943d515-1756-405e-a5b5-4d872c0038c9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-05T17:57:38.581461Z",
     "iopub.status.busy": "2023-02-05T17:57:38.581197Z",
     "iopub.status.idle": "2023-02-05T17:57:38.584833Z",
     "shell.execute_reply": "2023-02-05T17:57:38.584416Z",
     "shell.execute_reply.started": "2023-02-05T17:57:38.581448Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calc_accuracy(X, Y):\n",
    "    max_vals, max_indices = torch.max(X, 1)\n",
    "    train_acc = (max_indices == Y).sum().data.cpu().numpy() / max_indices.size()[0]\n",
    "    return train_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "298dca14-7927-4aad-a6a1-9fbea2c0ad2c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-06T03:58:48.173990Z",
     "iopub.status.busy": "2023-02-06T03:58:48.173782Z",
     "iopub.status.idle": "2023-02-06T04:00:51.108935Z",
     "shell.execute_reply": "2023-02-06T04:00:51.108138Z",
     "shell.execute_reply.started": "2023-02-06T03:58:48.173976Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total steps per epoch:  1612.875\n",
      "training on epoch:  0\n",
      "training on step:  50\n",
      "total time used is: 2.67 s\n",
      "training on step:  100\n",
      "total time used is: 5.29 s\n",
      "training on step:  150\n",
      "total time used is: 7.92 s\n",
      "training on step:  200\n",
      "total time used is: 10.56 s\n",
      "training on step:  250\n",
      "total time used is: 13.22 s\n",
      "training on step:  300\n",
      "total time used is: 15.87 s\n",
      "training on step:  350\n",
      "total time used is: 18.53 s\n",
      "training on step:  400\n",
      "total time used is: 21.19 s\n",
      "training on step:  450\n",
      "total time used is: 23.87 s\n",
      "training on step:  500\n",
      "total time used is: 26.56 s\n",
      "training on step:  550\n",
      "total time used is: 29.25 s\n",
      "training on step:  600\n",
      "total time used is: 31.93 s\n",
      "training on step:  650\n",
      "total time used is: 34.63 s\n",
      "training on step:  700\n",
      "total time used is: 37.34 s\n",
      "training on step:  750\n",
      "total time used is: 40.05 s\n",
      "training on step:  800\n",
      "total time used is: 42.76 s\n",
      "training on step:  850\n",
      "total time used is: 45.46 s\n",
      "training on step:  900\n",
      "total time used is: 48.18 s\n",
      "training on step:  950\n",
      "total time used is: 50.90 s\n",
      "training on step:  1000\n",
      "total time used is: 53.63 s\n",
      "training on step:  1050\n",
      "total time used is: 56.35 s\n",
      "training on step:  1100\n",
      "total time used is: 59.06 s\n",
      "training on step:  1150\n",
      "total time used is: 61.79 s\n",
      "training on step:  1200\n",
      "total time used is: 64.52 s\n",
      "training on step:  1250\n",
      "total time used is: 67.25 s\n",
      "training on step:  1300\n",
      "total time used is: 69.98 s\n",
      "training on step:  1350\n",
      "total time used is: 72.70 s\n",
      "training on step:  1400\n",
      "total time used is: 75.43 s\n",
      "training on step:  1450\n",
      "total time used is: 78.16 s\n",
      "training on step:  1500\n",
      "total time used is: 80.90 s\n",
      "training on step:  1550\n",
      "total time used is: 83.63 s\n",
      "training on step:  1600\n",
      "total time used is: 86.35 s\n",
      "training on step:  1650\n",
      "total time used is: 89.09 s\n",
      "training on step:  1700\n",
      "total time used is: 91.83 s\n",
      "training on step:  1750\n",
      "total time used is: 94.58 s\n",
      "training on step:  1800\n",
      "total time used is: 97.31 s\n",
      "training on step:  1850\n",
      "total time used is: 100.05 s\n",
      "training on step:  1900\n",
      "total time used is: 102.80 s\n",
      "training on step:  1950\n",
      "total time used is: 105.55 s\n",
      "training on step:  2000\n",
      "total time used is: 108.30 s\n",
      "training on step:  2050\n",
      "total time used is: 111.03 s\n",
      "training on step:  2100\n",
      "total time used is: 113.79 s\n",
      "training on step:  2150\n",
      "total time used is: 116.55 s\n",
      "training on step:  2200\n",
      "total time used is: 119.30 s\n",
      "training on step:  2250\n",
      "total time used is: 122.04 s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1383024/2650844576.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;31m# get outputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb_input_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mb_input_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mb_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m         \u001b[0;31m# get loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1173\u001b[0m         \u001b[0mreturn_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_return_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1175\u001b[0;31m         outputs = self.roberta(\n\u001b[0m\u001b[1;32m   1176\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1177\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    833\u001b[0m             \u001b[0mpast_key_values_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpast_key_values_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    834\u001b[0m         )\n\u001b[0;32m--> 835\u001b[0;31m         encoder_outputs = self.encoder(\n\u001b[0m\u001b[1;32m    836\u001b[0m             \u001b[0membedding_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    837\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextended_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    520\u001b[0m                 )\n\u001b[1;32m    521\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 522\u001b[0;31m                 layer_outputs = layer_module(\n\u001b[0m\u001b[1;32m    523\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    447\u001b[0m             \u001b[0mpresent_key_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpresent_key_value\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcross_attn_present_key_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m         layer_output = apply_chunking_to_forward(\n\u001b[0m\u001b[1;32m    450\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeed_forward_chunk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunk_size_feed_forward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseq_len_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m         )\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mapply_chunking_to_forward\u001b[0;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[1;32m   1999\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_chunks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchunk_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2000\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2001\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mforward_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput_tensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py\u001b[0m in \u001b[0;36mfeed_forward_chunk\u001b[0;34m(self, attention_output)\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    460\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfeed_forward_chunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 461\u001b[0;31m         \u001b[0mintermediate_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintermediate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    462\u001b[0m         \u001b[0mlayer_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mintermediate_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    463\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlayer_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    361\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 363\u001b[0;31m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintermediate_act_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training\n",
    "import time\n",
    "# Store the average loss after each epoch \n",
    "loss_values = []\n",
    "# number of total steps for each epoch\n",
    "print('total steps per epoch: ',  len(train_dataloader) / batch_size)\n",
    "# looping over epochs\n",
    "for epoch_i in range(0, epochs):\n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    print('training on epoch: ', epoch_i)\n",
    "    # set start time \n",
    "    t0 = time.time()\n",
    "    # reset total loss\n",
    "    total_loss = 0\n",
    "    train_acc = 0.0\n",
    "    # model in training \n",
    "    model.train()\n",
    "    # loop through batch \n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        # Progress update every 50 step \n",
    "        if step % 50 == 0 and not step == 0:\n",
    "            print('training on step: ', step)\n",
    "#             print('total time used is: {0:.2f} s'.format(time.time() - t0))\n",
    "        # load data from dataloader \n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].float().to(device)\n",
    "        # clear any previously calculated gradients \n",
    "        model.zero_grad()\n",
    "        # get outputs\n",
    "        outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
    "        # get loss\n",
    "        loss = outputs[0]\n",
    "        # total loss\n",
    "        total_loss += loss.item()\n",
    "        # clip the norm of the gradients to 1.0.\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        # update optimizer\n",
    "        optimizer.step()\n",
    "        # update learning rate \n",
    "        scheduler.step()\n",
    "        \n",
    "    # Calculate the average loss over the training data.\n",
    "    avg_train_loss = total_loss / len(train_dataloader)\n",
    "    # Store the loss value for plotting the learning curve.\n",
    "    loss_values.append(avg_train_loss)\n",
    "    print(\"average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "    \n",
    "    # Test\n",
    "    import numpy as np\n",
    "    t0 = time.time()\n",
    "    # model in validation mode\n",
    "    model.eval()\n",
    "    # save prediction\n",
    "    predictions,true_labels =[],[]\n",
    "    # evaluate data for one epoch\n",
    "    for batch in test_dataloader:\n",
    "        # Add batch to GPU\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        # Unpack the inputs from our dataloader\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        # validation\n",
    "        with torch.no_grad():\n",
    "            outputs = model(b_input_ids,\n",
    "                            token_type_ids=None,\n",
    "                            attention_mask=b_input_mask)\n",
    "        # get output\n",
    "        logits = outputs[0]\n",
    "        # move logits and labels to CPU\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "        final_prediction = np.argmax(logits, axis=-1).flatten()\n",
    "        label_num = np.argmax(label_ids, axis=-1).flatten()\n",
    "        predictions.append(final_prediction)\n",
    "        true_labels.append(label_num)\n",
    "\n",
    "    print('total time used is: {0:.2f} s'.format(time.time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "319b424e-ed87-4ba0-942d-73fc5f523481",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-06T03:57:33.453365Z",
     "iopub.status.busy": "2023-02-06T03:57:33.452934Z",
     "iopub.status.idle": "2023-02-06T03:57:33.458561Z",
     "shell.execute_reply": "2023-02-06T03:57:33.458098Z",
     "shell.execute_reply.started": "2023-02-06T03:57:33.453348Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['기타', '기타', '기타', ..., '기타', '기타', '기타'], dtype=object)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert numeric label to string\n",
    "final_prediction_list = label_encoder.inverse_transform(np.concatenate(predictions))\n",
    "final_truelabel_list = label_encoder.inverse_transform(np.concatenate(true_labels))\n",
    "\n",
    "final_prediction_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a51d25a9-9ec4-4e84-8483-81993b814ed8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-06T03:57:34.769660Z",
     "iopub.status.busy": "2023-02-06T03:57:34.769455Z",
     "iopub.status.idle": "2023-02-06T03:57:34.983608Z",
     "shell.execute_reply": "2023-02-06T03:57:34.983074Z",
     "shell.execute_reply.started": "2023-02-06T03:57:34.769646Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        경제금융       0.00      0.00      0.00      1493\n",
      "        교육과학       0.00      0.00      0.00       809\n",
      "        교통물류       0.00      0.00      0.00      1054\n",
      "          기타       0.00      0.80      0.00         5\n",
      "        문화관광       0.00      0.00      0.00      1291\n",
      "        보건의료       0.00      0.00      0.00       457\n",
      "        사회복지       0.08      0.27      0.12       390\n",
      "        식품건강       0.00      0.00      0.00       798\n",
      "        재난안전       0.01      0.00      0.01       618\n",
      "        제조소비       0.00      0.00      0.00      1119\n",
      "        주택토지       0.00      0.00      0.00       283\n",
      "      통일외교안보       0.00      0.00      0.00        33\n",
      "        행정법률       0.00      0.00      0.00      1965\n",
      "        환경기상       0.00      0.00      0.00      1186\n",
      "\n",
      "    accuracy                           0.01     11501\n",
      "   macro avg       0.01      0.08      0.01     11501\n",
      "weighted avg       0.00      0.01      0.00     11501\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/keti/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/keti/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/keti/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "cr = classification_report(final_truelabel_list, \n",
    "                           final_prediction_list, \n",
    "                           output_dict=False)\n",
    "print(cr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "58a7b48c-7a76-4953-9fe0-5edc66166ff8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-06T03:43:00.362901Z",
     "iopub.status.busy": "2023-02-06T03:43:00.362662Z",
     "iopub.status.idle": "2023-02-06T03:43:01.072418Z",
     "shell.execute_reply": "2023-02-06T03:43:01.071875Z",
     "shell.execute_reply.started": "2023-02-06T03:43:00.362886Z"
    }
   },
   "outputs": [],
   "source": [
    "torch.save(model, \"roberta_small.pt\")\n",
    "torch.save(model.state_dict(),\"roberta_small_state_dict.pt\")\n",
    "torch.save({\"model\": model.state_dict(), \"optimizer\": optimizer.state_dict()},\"roberta_small_all.tar\",)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad65e03-6981-42e7-8ee2-888c95b512a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
